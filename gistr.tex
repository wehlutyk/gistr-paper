\documentclass[a4paper,fleqn]{cas-dc}

%\usepackage[authoryear,longnamesfirst]{natbib}
\usepackage[authoryear]{natbib}
%\usepackage[numbers]{natbib}

% To make comments, corrections, notes
\newcommand{\tb}[1]{\textcolor{blue}{#1}}
\newcommand{\rk}[1]{\tb{{\footnotesize {\bf[\emph{#1}]}}}}
\newcommand{\marginnote}[1]{\marginpar{{\parbox{1.\linewidth}{\hrulefill\\\tb{\scriptsize \sf #1}}}}}
\newcommand{\cam}[1]{\tb{\small{[{\bf C:} }#1{]}}}
\newcommand{\seb}[1]{\tb{\small{[{\bf S:} #1{]}}}}

\begin{document}
%\let\WriteBookmarks\relax
%\def\floatpagepagefraction{1}
\def\textpagefraction{.001}
%% TODO: define a proper short title
\shorttitle{Gistr}
%\shortauthors{}

\title[mode=title]{Transformations in large-scale text transmission chains}

\author[1]{Sébastien Lerique}[orcid=0000-0002-5787-8397]
\cormark[1]
\ead{sebastien.lerique@normalesup.org}
\ead[url]{slvh.fr}
\credit{TODO: credits}
\address[1]{Embodied Cognitive Science Unit, Okinawa Institute of Science and Technology Graduate University, Onna-son, Okinawa 904-0495, Japan}

% TODO[cam]: add orcid id
\author[2,3]{Camille Roth}
\ead{roth@cmb.hu-berlin.de}
\ead[URL]{camilleroth.eu}
\credit{TODO: credits}
\address[2]{Camille's first affiliation}
\address[3]{Camille's second affiliation}

\cortext[cor1]{Corresponding author}

\begin{abstract}
  \rk{TODO: abstract}
\end{abstract}

%% TODO: graphical abstract
%% \begin{graphicalabstract}
%% \includegraphics{figs/grabs.pdf}
%% \end{graphicalabstract}

%% TODO: highlights
%% \begin{highlights}
%% \item Research highlights item 1
%% \item Research highlights item 2
%% \item Research highlights item 3
%% \end{highlights}

\begin{keywords}
  TODO: keywords \sep more keywords
\end{keywords}

\maketitle


\section{Introduction}\label{sec:gistr-intro}

The previous chapter demonstrated that it is possible to observe
cognitive biases in the way quotations are copied from blog to blog. By
grounding those biases in known effects in the recall of word lists, we
also showed that the enquiry of cultural evolution for linguistic
content can be related to lower-level cognitive mechanisms that help
understand the way content is transformed in ecological situations.
However in the online corpus we considered only extremely simple
transformations, namely individual word replacements, so as to be able
to infer missing links between quotations and thus make the analysis
possible. While we observed a reliable bias in the way words are
replaced, consistent with known psycholinguistic biases, our view of the
overall transformations is extremely narrow: not only is it restricted
to word replacements, it is limited to the replacements that were the
only change in an utterance (other than sentence cropping). The analysis
also remained at the low-level of lexical properties such as word
frequency and age of acquisition, neither of which give much insight
into the semantic changes that utterances can undergo. Finally,
constraints of the data did not let us identify chains of
transformations, and we could not observe the evolution of quotations
beyond the individual transformation step.

We now wish to remedy most of these points by studying the evolution of
short utterances in a controlled experimental setting. A controlled
setting means having a less ecological situation, but also allows for
the collection of all the available data for analysis. Once again, our
approach is exploratory, and we aim to reach a more complete
understanding of the transformation process that is at work in the
propagation of online quotations, but also more widely in the evolution
of written utterances as they are transmitted using other mediums. Our
reasoning is that by better understanding the transformations undergone
by such utterances, we will gain insight into the way actual linguistic
representations may change because of interpretation and memory
mechanisms.

More precisely, our goal is to construct a descriptive model of the
process that can bring insight into why utterances change the way they
do, and how such observations can be connected to current knowledge in
linguistics, on one side, and to the broader cultural evolution
frameworks, on the other. Indeed, current knowledge of the
transformation of utterances is quite partial: laboratory transmission
chains show that a number of high-level biases appear in the
transmission of purposefully constructed complex stories, but do not
explain in detail how such trends come about. On the other hand, the
psycholinguistics literature on sentence recall shows that there are
important semantic and syntactic effects in the way sentences are
reformulated, but they do so on extremely simple types of content that
make it difficult to generalise results. There seems to be a missing
link between the high-level effects observed in
transmission chains of complex stories and the lower-level processes known to act in the recall of simple sentences. A descriptive
model of transformations would go a long way in creating this link, and would thus help to explain more easily the overall evolution of utterances in terms of lower-level cognitive mechanisms.

One of the ideal setups to tackle this question would be a standard transmission
chain where we observe the accumulated transformations made by
participants on a set of utterances that we choose.
%However, given our exploratory approach and the fact that we do not know in advance what the model will look like, it is important that we can run several such experiments in short cycles so as to adjust the task parameters and the sampling of utterances. It is also important that the collected data be of similar size to the number of substitutions we extracted in the previous chapter, so that we will be able to compare and validate any overlapping results.
We therefore decided to run a set of transmission chain experiments on an online platform developed for the purpose: as we shall see, after an initial development phase this approach lets us collect large amounts of data in short periods of time, while maintaining a level of control similar to that of laboratory experiments.

We begin by discussing the works relevant to this endeavour, and in
particular the bind in which current transmission chain experiments on
linguistic content find themselves. We then present the procedure
followed to develop the online experimental platform, and the measures
implemented to achieve a high level of quality in the data. Next, we
expose our analysis of the data sets collected, present the descriptive
model of transformations we construct from them, and highlight the main
behaviours the model lets us observe. Finally, we discuss the relevance
of these results in the broader context of the study of cultural
evolution.

\section{Related work}\label{sec:gistr-related}

Inspired by the selectionist models of culture developed by
\citet{boyd_culture_1985} and
\citet{cavalli-sforza_cultural_1981}, a sizeable part of the
empirical work on cultural change has focused on identifying and separating content and
context biases in the way cultural items are transmitted. This line of
work relies heavily on the transmission chain paradigm initially
introduced by \citet{bartlett_remembering:_1995}. For linguistic
content in particular, studies using that paradigm now provide a
catalogue of contrasts in the way utterances or short stories are
transmitted. These effects range from the stereotypical personification
of objects \citep{bangerter_transformation_2000}, the favouring of
negative story aspects \citep{bebbington_sky_2017} or the increased
hierarchical encoding of events \citep{mesoudi_hierarchical_2004}, to
biases in favour of social \citep{mesoudi_bias_2006} or
counter-intuitive aspects of stories
\citep{norenzayan_memory_2006,barrett_spreading_2001}. Other
effects such as the role of emotions in the selection of items to
reproduce \citep{heath_emotional_2001,eriksson_corpses_2014}, or
conformity and prestige biases \citep{acerbi_did_2017} have been
studied by focusing on the individual transmission step on which the
evolution of content hinges.

Often, such effects are identified by selecting two or more minimally
different types of content and contrasting the way they evolve in
transmission chains (for instance measuring the rate at which they are
degraded). When a type of content is significantly better transmitted
than other types, it signals that a bias is acting on that contrast
dimension. The technique is useful in the context of selectionist models
of culture, as it identifies examples of biases which could create
selection pressures for specific cultural types and thus drive cultural
evolution. It is also relevant to the Cultural Attraction framework,
which focuses on the aspects of culture for which reconstructive
processes are more important than selection. For instance, the approach
recently introduced by \citet{claidiere_how_2014} proposes to
use evolutionary causal matrices to model such attraction-based
processes in cultural evolution, and could gain insight from the trends
observed in transmission chains.
%In the terminology of
%\citet{morin_how_2016}, selectionist models focus on how culture
%survives in spite of wear-and-tear, and cultural attraction focuses on
%how culture survives in spite of possible flops, where a given item
%fails to elicit sufficient interest to be recreated at all.
In theory,
both selection and attraction can be observed in transmission chains. However, in
its current implementation focused on contrasting outcomes between
several (usually two) conditions, the technique gives little insight
into the underlying mechanisms at work, and into how exactly such processes
can be described and explained in terms of cognitive %and situated
processing.

Indeed, understanding the mechanisms behind transformations in chains, or even only quantitatively describing the details of said transformations, remains very much a challenge.  This is especially true in the linguistic domain where the complexity of language hinders most attempts to comprehend sentence evolution. Now the current literature features two typical strategies to study such transformations. The first one trivially consists in doing the analysis by hand.  Here, we may further distinguish approaches relying on in vitro content, where data is produced by ad hoc experiments whose participants are asked to reformulate content under certain conditions, from approaches using in vivo content, usually based on large text datasets.
For instance, the study of the propagation of risk perception developed by \citet{moussaid_amplification_2015} used an interaction setting where subjects were filmed while freely discussing a topic. The recorded conversations were later hand-coded for the presence of certain information items introduced at the beginning of the chains. On the in vivo side, \citet{lauf_analyzing_2013} carried out a linguistic analysis of transformations of quotes in a corpus of news stories by exhaustively hand-coding differences between sentences.

The second strategy consists in using automated methods.  In the in vivo case, this generally requires to focus on a tightly constrained type of transformation, and possibly rely on some empirical proxies, as there is no control over the conditions of production of the observed sentences. %It also relies on pre-labelled data sets, often from online platforms, on which machine learning techniques can extract features that correlate to the transmission of pieces of content.
For instance, our recent analysis of single word substitutions in quotations observed in a large blog post dataset \citep{lerique-2018-semantic-drift} belongs to this category.
In a slightly different setting, \citet{danescu-niculescu-mizil_you_2012} study the memorability of movie quotes by exploiting user ratings provided on the Internet Movie Database website. While their study does not focus on the transformation of content per se, it illustrates the types of memory-related features that may also help explain transformations.

In an in vitro setting, linguistic content evolution is found in the study of intrusions in the recall of word lists \citep[see][for a review]{zaromb_temporal_2006}, and in the study of word replacements and simple syntactic changes in short sentences \citep{potter_regeneration_1990,lombardi_regeneration_1992}.
%the observation of linguistic content evolution can be found in the study of intrusions in the recall of word lists \citep[see][for a review]{zaromb_temporal_2006}, and in the study of word replacements and simple syntactic changes in short sentences \citep{potter_regeneration_1990,lombardi_regeneration_1992}.
These are much simpler transformations than transformations of complete sentences, and are thus more amenable to statistical analysis. Note that a similar focus on lower-dimensional changes is found in non-linguistic studies, such as iterated learning on sequences of colour items for which standard regularity metrics exist \citep{cornish_systems_2013}, or transmission chains of constrained visual patterns such as those used by \citet{claidiere_cultural_2014}. Both cases feature discrete and combinatorial pieces of content, for which it is possible to use standard notions of distance, equality, or regularity in transformation.



%In an in vitro setting, the only type of observed linguistic content evolution relies on single words –-- either in the case of the study of intrusions in the recall of word lists \citep[see][for a review]{zaromb_temporal_2006}, and that of word replacements in short sentences \citep{potter_regeneration_1990,lombardi_regeneration_1992}
%%These studies can be seen as employing that strategy:
%--- word intrusions in lists and individual replacements in sentences
%both processes are much simpler than transformations of complete sentences, and are thus more amenable to statistical analysis. To the best of our knowledge, no experimental study has so far described sophisticated transformations at the sentence level in an automated manner.
%%In other words, automated analysis of in vitro content tightly constrains the linguistic features, for instance sentences of a very specific type, or for which only pre-defined changes can happen. %In that case the transformations can be directly modelled to identify regularities.



%%Indeed, understanding the mechanisms behind transformations in chains,
%%or even only quantitatively describing the details of said
%%transformations, remains very much a challenge. This is especially true
%%in the linguistic domain, where the complexity of language hinders most
%%attempts to understand what is going on in a transformation. Up to now
%%three main strategies have been developed to delve into to detail of
%%transformations. The first is to use tightly constrained linguistic
%%content, for instance sentences of a very specific type, or for which
%%only pre-defined changes can happen. In that case the transformations
%%can be directly modelled to identify regularities. The study of
%%intrusions in the recall of word lists \citep[see][for a
%%review]{zaromb_temporal_2006}, and that of word replacements in short
%%sentences
%%\citep{potter_regeneration_1990,lombardi_regeneration_1992}, can be
%%seen as employing that strategy: word intrusions in lists and individual
%%replacements in sentences are much simpler than transformations of
%%complete sentences, and are thus more amenable to statistical analysis.
%%Our analysis of single word substitutions in quotations observe in a large blog post dataset \citep{lerique-2018-semantic-drift} can be categorised here too. A similar strategy is found in non-linguistic
%%studies, such as iterated learning on sequences of colour items for
%%which standard regularity metrics exist \citep{cornish_systems_2013},
%%or transmission chains of constrained visual patterns such as those used
%%by \citet{claidiere_cultural_2014}. Both cases feature discrete and
%%combinatorial pieces of content, for which it is possible to use natural
%%notions of distance, equality, or regularity in transformation. This
%%first strategy can be termed the \enquote{simple setting} strategy.
%%
%%At the other end of the spectrum we find the \enquote{do-it-by-hand}
%%strategy. This approach uses more ecological content but relies on
%%exhaustively hand-coding it, and is used in most of the transmission
%%chain studies mentioned above. The study of risk perception propagation
%%developed by \citet{moussaid_amplification_2015}, for instance, used
%%a free-form interaction setting where subjects were taped while freely
%%discussing a topic. The recorded conversations were later hand-coded for
%%the presence of certain information items introduced at the beginning of
%%the chains. The linguistic analysis of transformations of quotes in news
%%stories provided by \citet{lauf_analyzing_2013} is also the product
%%of exhaustively hand-coding differences between sentences.
%%
%%Finally, the third strategy relies on pre-labelled data sets, often from
%%online platforms, on which machine learning techniques can extract
%%features that correlate to the transmission of pieces of content. This
%%is the \enquote{already coded} strategy.
%%\citet{danescu-niculescu-mizil_you_2012}, for instance, study the
%%memorability of movie quotes by exploiting user ratings provided on the
%%Internet Movie Database website. While their study does not focus on the
%%transformation of content per se, it illustrates the types of
%%memory-related features that pre-labelled data sets can help extract in
%%order to explain transformations. Conversely, analysing the regularities
%%that arise in unlabelled digital traces often falls back into the first
%%strategy: without an outsourced coding of the data, analysing
%%unconstrained real-life interactions is so complex that one often has to
%%focus on a limited set of features in the available data, thus proposing
%%a \enquote{simple setting} analysis. As we saw in the previous chapter,
%%the analysis of partial digital traces can also fall back into the first
%%strategy, as having to infer missing information led to drastically
%%simplifying the transformations considered.
%%
%%Strategies two and three are additionally closely tied to data
%%collection methods. Free-form interaction and more generally ecological
%%content is costly to hand-code, and thus necessarily limited in size; it
%%is also best used in controlled settings where the choice of content can
%%be optimised. Conversely, using machine learning to extract features
%%that relate to content transmission requires large amounts of
%%pre-labelled data, which often means that an existing public data set
%%must be used. Such studies thus seldom control the conditions under
%%which the data is generated, which restricts the interactions they can
%%explore to those encoded in existing data sets: any behaviour or piece
%%of content that is not observable in public data sets is off limits.

%%Overall, studies targeted at understanding the details of
%%transformations of linguistic content seem forced to pick two of the
%%following three properties, and relinquish the third: realistic content,
%%computational analysis, and control over the generation of the data.
%%Picking \enquote{realistic content and computational analysis} leads to
%%the \enquote{already coded} strategy. Picking \enquote{realistic content
%%and control over data generation} requires hand-coding a substantial
%%part of the data collected, that is strategy two. Finally,
%%\enquote{computational analysis and data generation control} leads to
%%the \enquote{simple setting} strategy. This bind thus appears as a major
%%challenge to the better understanding of changes in linguistic content,
%%and more broadly to the study of language-related cultural evolution. In
%%particular, it hinders attempts to model the low-level processes which
%%could provide a more parsimonious account of the contrasts observed in
%%linguistic transmission chains, and allow for a deeper integration of
%%the study of cultural evolution with linguistics.

To the best of our knowledge, no experimental study so far has described sophisticated transformations at the sentence level in an automated manner.
To overcome this obstacle we turn to two related fields of research. The
first, which we term the Web and Smartphone experimental approach, %is creating a middle ground between controlled laboratory experiments and the analysis of online corpora. This approach
takes advantage of the ubiquity of internet browsers and mobile computing to develop
large-scale controlled experiments out of the laboratory.
\citet{miller_smartphone_2012} %discusses the possibilities opened by developing experiments as smartphone applications in particular, and
notes that this method changes the logistics %and context-awareness
of experiments: large amounts of subjects can be recruited online without
having to manage meeting schedules, %, and experiments can probe
%participants without interrupting their everyday life, both
an advantage
that has been exploited in the study of mind-wandering and happiness
\citep{killingsworth_wandering_2010,mackerron_happiness_2013,bastian_language_2017},
%A closely related method is the development of experiments as web
%applications, which similarly changes the set of experimental
%constraints.
%In linguistics, the possibility for large-scale data
%collection has been successfully used in
 vocabulary size
\citep{keuleers_word_2015,brysbaert_how_2016}, %; creating studies
%that involve many subjects at the same time is also made much simpler by
%the online logistics, an advantage that has been used for instance in
%the study of
or group conversations \citep[][which also involves many subjects simultaneously]{niculae_conversational_2016}.
%More generally, these approaches relax the opposition between
%small-scale controlled experiments in the laboratory on one side, and
%analyses of large-scale but passively collected online data on the other
%side.
Once the initial development cost is covered, this approach makes it
possible to collect relatively large data sets in short cycles, and
combines simplified logistics with a level of control similar to that of
laboratory experiments.

The second field we rely on creates an opening for the detailed
modelling of utterance transformations: biological sequence alignment,
the sub-field of bioinformatics which seeks to uncover commonalities in
sequences of DNA, RNA, or amino acids in proteins from different
species, has developed over the last 50 years a range of general
algorithms to relate sequences of items. One such algorithm in
particular, introduced by \citet{needleman_general_1970}, extends the
principles of the Levenshtein distance and is particularly well suited
to the analysis of linguistic transformations when combined with
standard natural language processing methods. Inspired by
\citet{lauf_analyzing_2013} who use similar tools to prepare their
data for manual analysis, we use and extend the Needleman-Wunsch
algorithm to reliably extract regularities in the way utterances are
transformed through transmission chains.


%% -----------------------------------------------------------------------------------------------------------------------------------------


\printcredits

\section*{Acknowledgements}
\rk{todo}

\section*{Software colophon}
\rk{todo}

%% Loading bibliography style file
\bibliographystyle{cas-model2-names}

% Loading bibliography database
\bibliography{gistr}

\end{document}
